timestamp,model_dir,base_model,lora_r,lora_alpha,lora_dropout,lora_bias,target_modules,num_epochs,batch_size,max_length,gradient_clip,learning_rate,optimizer,weight_decay,scheduler,dataset,training_time_minutes
20250308045220,20250308_vjg_alpha16_dropout0.05_r8_lr0.0003_epochs3,codellama/CodeLlama-7b-hf,8,16,0.05,none,"q_proj,v_proj",3,4,512,1.0,0.0003,adamw,0.01,cosine,code_search_net,2.62
20250308045546,20250308_czb_alpha16_dropout0.05_r8_lr0.0003_epochs3,codellama/CodeLlama-7b-hf,8,16,0.05,none,"q_proj,v_proj",3,4,512,1.0,0.0003,adamw,0.01,cosine,code_search_net,2.58
20250308045833,20250308_czb_alpha16_dropout0.05_r8_lr0.0002_epochs3,codellama/CodeLlama-7b-hf,8,16,0.05,none,"q_proj,v_proj",3,4,512,1.0,0.0002,adamw,0.01,cosine,code_search_net,2.59
